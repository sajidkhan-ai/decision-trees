ğŸŒ± Decision Trees: From Basics to Pruning

When I first started exploring Decision Trees, I was fascinated by how a simple "yes/no" style of questioning could break down even complex datasets. It almost felt like the algorithm was playing â€œ20 Questionsâ€ with the data.

So, I picked the Iris dataset â€” the classic playground for machine learning beginners. At first, I trained a Decision Tree without any restrictions, and as expected, it tried to grow as much as possible. But soon, I noticed something interesting:

At deeper levels, the tree started creating sub-branches even when the decision was already clear.

For example, if one branch had values like [0, 36, 1], the majority class was obvious â€” we didnâ€™t need more splits.

Thatâ€™s when I learned about Post-pruning ğŸŒ³âœ‚ï¸

Instead of letting the tree grow endlessly, I introduced the hyperparameter max_depth. By controlling how deep the tree could go, I was able to simplify the model without losing accuracy â€” in fact, it made the tree more interpretable and avoided overfitting.

ğŸš€ Whatâ€™s inside this repo?

Decision Tree Classifier on Iris dataset â€“ with and without pruning.

Hands-on exploration of max_depth and its impact.

A step towards model generalization and avoiding overfitting.

ğŸ”® Whatâ€™s next?

This is just the beginning. Iâ€™ll be experimenting with:

Other hyperparameters like min_samples_split, min_samples_leaf, etc.

Applying Decision Trees on a regression problem.

Comparing results across datasets to see how pruning strategies differ.

ğŸ“ Why I built this

I believe learning ML isnâ€™t just about applying algorithms â€” itâ€™s about understanding their behavior. By experimenting, observing, and storytelling, Iâ€™m making my journey more fun and hopefully inspiring others who are also starting out.
